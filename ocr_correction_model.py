# -*- coding: utf-8 -*-
"""OCR CORRECTION MODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fHKuLF9fOGxVU5DXK_q09Bf8DitVeufa

COMPLETE WORKING SEQUENCE FOR GOOGLE COLAB
===========================================

CRITICAL FIRST STEP:
Go to Runtime ‚Üí Restart Runtime
Then run these cells in order
"""

print("üîß Installing compatible packages...")
!pip uninstall -y transformers accelerate peft tokenizers -q
!pip install -q transformers==4.36.0 accelerate==0.25.0 tokenizers==0.15.0 \
              sentencepiece datasets scikit-learn kaggle

import torch
import transformers
print(f"‚úÖ PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}")
print(f"‚úÖ Transformers: {transformers.__version__}")

from transformers import Seq2SeqTrainer
print(f"‚úÖ Trainer imports working!")

# ============================================================================
# CELL 2: Upload Kaggle Token
# ============================================================================

from google.colab import files
uploaded = files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
print("‚úÖ Kaggle API configured!")

# ============================================================================
# CELL 3: Download Datasets
# ============================================================================

import os, shutil

# Clean old files
for path in ['vyakaran_data', 'sanskrit_corpus_data', 'vyakaran.zip', 'sanskrit-text-corpus.zip']:
    if os.path.exists(path):
        shutil.rmtree(path) if os.path.isdir(path) else os.remove(path)

# Download
!kaggle datasets download -d aluminium13/vyakaran --force
!kaggle datasets download -d kartikbhatnagar18/sanskrit-text-corpus --force

# Extract
!unzip -o -q vyakaran.zip -d vyakaran_data
!unzip -o -q sanskrit-text-corpus.zip -d sanskrit_corpus_data
!rm *.zip

print("‚úÖ Datasets ready!")
!ls vyakaran_data
!ls sanskrit_corpus_data

# ============================================================================
# CELL 4: OCR Error Simulator
# ============================================================================

import random

class OCRErrorSimulator:
    def __init__(self):
        self.char_substitutions = {
            '‡§ï': ['‡§ñ', '‡§ó'], '‡§ö': ['‡§õ', '‡§ú'], '‡§ü': ['‡§†', '‡§°'],
            '‡§§': ['‡§•', '‡§¶'], '‡§™': ['‡§´', '‡§¨'], '‡§æ': ['‡•ã', '‡•å'],
            '‡§ø': ['‡•Ä', '‡•á'], '‡•Å': ['‡•Ç', '‡•ã'],
        }

    def add_spelling_errors(self, text, error_rate=0.1):
        chars = list(text)
        for _ in range(int(len(chars) * error_rate)):
            idx = random.randint(0, len(chars) - 1)
            if chars[idx] in self.char_substitutions:
                chars[idx] = random.choice(self.char_substitutions[chars[idx]])
        return ''.join(chars)

    def add_spacing_errors(self, text, error_rate=0.15):
        words = text.split()
        if len(words) <= 1:
            return text
        if random.random() < 0.5:
            for _ in range(int(len(words) * error_rate)):
                if len(words) > 1:
                    idx = random.randint(0, len(words) - 2)
                    words[idx] = words[idx] + words[idx + 1]
                    words.pop(idx + 1)
        else:
            result = []
            for word in words:
                if len(word) > 3 and random.random() < error_rate:
                    split_pos = random.randint(1, len(word) - 1)
                    result.extend([word[:split_pos], word[split_pos:]])
                else:
                    result.append(word)
            words = result
        return ' '.join(words)

    def jumble_words(self, text, error_rate=0.1):
        words = text.split()
        if len(words) <= 1:
            return text
        for _ in range(int(len(words) * error_rate)):
            if len(words) > 1:
                idx = random.randint(0, len(words) - 2)
                words[idx], words[idx + 1] = words[idx + 1], words[idx]
        return ' '.join(words)

    def add_extra_chars(self, text, error_rate=0.05):
        chars = list(text)
        for _ in range(int(len(chars) * error_rate)):
            idx = random.randint(0, len(chars) - 1)
            chars.insert(idx, chars[idx])
        return ''.join(chars)

    def simulate_ocr_errors(self, text, error_level='medium'):
        rates = {
            'low': (0.05, 0.08, 0.05, 0.03),
            'medium': (0.10, 0.15, 0.10, 0.05),
            'high': (0.15, 0.20, 0.15, 0.08)
        }
        spell, space, jumble, extra = rates.get(error_level, rates['medium'])

        noisy = text
        if random.random() < 0.8:
            noisy = self.add_spelling_errors(noisy, spell)
        if random.random() < 0.7:
            noisy = self.add_spacing_errors(noisy, space)
        if random.random() < 0.3:
            noisy = self.jumble_words(noisy, jumble)
        if random.random() < 0.5:
            noisy = self.add_extra_chars(noisy, extra)
        return noisy

print("‚úÖ OCR Error Simulator ready!")

# ============================================================================
# CELL 5: Dataset Class
# ============================================================================

from torch.utils.data import Dataset

class SanskritOCRDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128, is_training=True):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.is_training = is_training
        self.error_simulator = OCRErrorSimulator()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        clean = self.texts[idx]
        noisy = self.error_simulator.simulate_ocr_errors(clean) if self.is_training else clean

        inputs = self.tokenizer(noisy, max_length=self.max_length,
                               padding='max_length', truncation=True, return_tensors='pt')
        labels = self.tokenizer(clean, max_length=self.max_length,
                               padding='max_length', truncation=True, return_tensors='pt')

        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': labels['input_ids'].squeeze()
        }

print("‚úÖ Dataset class ready!")

"""change the all_texts from 5000 to 100000"""

# ============================================================================
# CELL 6: Load Data
# ============================================================================

import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split

print("üìä Loading data...")

all_texts = []

# Load Vyakaran
for file in Path('vyakaran_data').glob('*.csv'):
    try:
        df = pd.read_csv(file)
        for col in df.columns:
            all_texts.extend(df[col].dropna().astype(str).tolist())
        print(f"‚úì {file.name}")
    except Exception as e:
        print(f"‚úó {file.name}: {e}")

# Load Sanskrit Corpus
for file in list(Path('sanskrit_corpus_data').glob('*.csv')) + list(Path('sanskrit_corpus_data').glob('*.txt')):
    try:
        if file.suffix == '.csv':
            df = pd.read_csv(file)
            for col in df.columns:
                all_texts.extend(df[col].dropna().astype(str).tolist())
        else:
            with open(file, 'r', encoding='utf-8') as f:
                all_texts.extend([line.strip() for line in f if line.strip()])
        print(f"‚úì {file.name}")
    except Exception as e:
        print(f"‚úó {file.name}: {e}")

# Clean and deduplicate
all_texts = [t.strip() for t in all_texts if len(t.strip()) > 5]
all_texts = list(set(all_texts))

print(f"\n‚úÖ Total: {len(all_texts):,} sentences")

# IMPORTANT: Take a sample for faster training
# Remove this line if you want to use all data
all_texts = all_texts[:100]  # Use 50k samples for manageable training time                              CHANGED
print(f"üì¶ Using sample: {len(all_texts):,} sentences")

# Split
train_texts, val_texts = train_test_split(all_texts, test_size=0.1, random_state=42)
print(f"üìä Train: {len(train_texts):,}, Val: {len(val_texts):,}")

# ============================================================================
# CELL 7: Load Model
# ============================================================================

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

print("ü§ñ Loading model...")
model_name = 'google/mt5-small'
tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

print(f"‚úÖ Model loaded! Parameters: {model.num_parameters():,}")

# ============================================================================
# CELL 8: Create Datasets
# ============================================================================

print("üì¶ Creating datasets...")
train_dataset = SanskritOCRDataset(train_texts, tokenizer, 128, True)
val_dataset = SanskritOCRDataset(val_texts, tokenizer, 128, True)
print(f"‚úÖ Train: {len(train_dataset):,}, Val: {len(val_dataset):,}")

# ============================================================================
# CELL 9: Setup Training
# ============================================================================

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq

print("‚öôÔ∏è Setting up training...")

training_args = Seq2SeqTrainingArguments(
    output_dir='./sanskrit_ocr_model',
    num_train_epochs=5,  # Reduced from 10
    per_device_train_batch_size=8,  # Reduced from 16 to prevent OOM
    per_device_eval_batch_size=8,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=100,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=2,
    load_best_model_at_end=True,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to='none',
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),
)

print("‚úÖ Training configured!")
print(f"üìã Epochs: {training_args.num_train_epochs}, Batch: {training_args.per_device_train_batch_size}")

# ============================================================================
# CELL 10: Train
# ============================================================================

print("üöÄ Starting training...")
print("=" * 80)
print("With 50k samples, this will take ~1-2 hours on T4 GPU")
print("=" * 80)

trainer.train()

print("\n‚úÖ TRAINING COMPLETE!")

# Save
model.save_pretrained('./sanskrit_ocr_model')
tokenizer.save_pretrained('./sanskrit_ocr_model')
print("üíæ Model saved!")

# ============================================================================
# CELL 11: Test
# ============================================================================

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

print("üß™ Loading and testing model...")

tokenizer = AutoTokenizer.from_pretrained('./sanskrit_ocr_model')
model = AutoModelForSeq2SeqLM.from_pretrained('./sanskrit_ocr_model')
model.to('cuda' if torch.cuda.is_available() else 'cpu')
model.eval()

def correct_text(text):
    inputs = tokenizer(text, return_tensors='pt', max_length=128,
                      padding='max_length', truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=128, num_beams=5)

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test
simulator = OCRErrorSimulator()
tests = ["‡§∞‡§æ‡§Æ‡§É ‡§µ‡§®‡§Ç ‡§ó‡§ö‡•ç‡§õ‡§§‡§ø", "‡§ï‡•É‡§∑‡•ç‡§£‡§É ‡§ó‡•Ä‡§§‡§æ‡§Ç ‡§Ö‡§µ‡§¶‡§§‡•ç"]

print("\n" + "=" * 80)
for original in tests:
    noisy = simulator.simulate_ocr_errors(original)
    corrected = correct_text(noisy)
    print(f"Original:  {original}")
    print(f"Noisy:     {noisy}")
    print(f"Corrected: {corrected}\n")
print("=" * 80)

# Check if training actually completed
import os
import json

if os.path.exists('./sanskrit_ocr_model/trainer_state.json'):
    with open('./sanskrit_ocr_model/trainer_state.json', 'r') as f:
        state = json.load(f)
    print(f"Training steps completed: {state.get('global_step', 0)}")
    print(f"Last loss: {state['log_history'][-1] if state.get('log_history') else 'None'}")
else:
    print("‚ùå No training state found - model didn't train!")

# ============================================================================
# CELL 12: Download Model
# ============================================================================

from google.colab import files
import shutil

print("üì• Packaging model...")
shutil.make_archive('sanskrit_ocr_model', 'zip', './sanskrit_ocr_model')
files.download('sanskrit_ocr_model.zip')
print("‚úÖ Downloaded!")

