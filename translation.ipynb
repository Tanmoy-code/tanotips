{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14168792,"sourceType":"datasetVersion","datasetId":9031508},{"sourceId":14168811,"sourceType":"datasetVersion","datasetId":9031516},{"sourceId":14168933,"sourceType":"datasetVersion","datasetId":9031598}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install Libraries\n!pip install -q transformers[torch] datasets evaluate sacrebleu sentencepiece accelerate\n!pip install protobuf==3.20.3\nprint(\"‚úì Libraries installed.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:55:10.174576Z","iopub.execute_input":"2025-12-15T17:55:10.175665Z","iopub.status.idle":"2025-12-15T17:55:17.898149Z","shell.execute_reply.started":"2025-12-15T17:55:10.175628Z","shell.execute_reply":"2025-12-15T17:55:17.897376Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n‚úì Libraries installed.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 2: Imports\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    Seq2SeqTrainingArguments, \n    Seq2SeqTrainer, \n    DataCollatorForSeq2Seq\n)\nfrom datasets import load_dataset, DatasetDict\nimport evaluate\nimport numpy as np\n\n# Setup Device (GPU or CPU)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚úì Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:55:25.460508Z","iopub.execute_input":"2025-12-15T17:55:25.461262Z","iopub.status.idle":"2025-12-15T17:55:25.466837Z","shell.execute_reply.started":"2025-12-15T17:55:25.461228Z","shell.execute_reply":"2025-12-15T17:55:25.465858Z"}},"outputs":[{"name":"stdout","text":"‚úì Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 3: Clean and Load Saamayik Dataset (Final Fix)\nimport pandas as pd\nimport re\nfrom datasets import Dataset, DatasetDict\nimport os\n\n# Define your file names (Make sure these match what you uploaded)\ndata_dir = \"/kaggle/input/coustom\"\nen_file = os.path.join(data_dir, \"dev.en\")\nsa_file = os.path.join(data_dir, \"dev.sa\")\n\ndef clean_line(line):\n    # 1. Remove the tags\n    #    We use a raw string for regex, but careful with backslashes\n    #    This pattern finds \"', '', line)\n    \n    # 2. Remove the triple quotes (\"\"\") that make code look commented\n    line = line.replace('\"\"\"', '')\n    \n    # 3. Remove leading/trailing whitespace\n    return line.strip()\n\nprint(\"Processing Saamayik dataset...\")\n\ntry:\n    # Check if files exist\n    if not os.path.exists(en_file) or not os.path.exists(sa_file):\n        raise FileNotFoundError(f\"Could not find {en_file} or {sa_file}. Please upload them!\")\n\n    # Read files\n    with open(en_file, \"r\", encoding=\"utf-8\") as f:\n        en_lines = f.readlines()\n        \n    with open(sa_file, \"r\", encoding=\"utf-8\") as f:\n        sa_lines = f.readlines()\n\n    # Create data pairs\n    data = []\n    # Zip combines the two lists line-by-line\n    for en, sa in zip(en_lines, sa_lines):\n        clean_en = clean_line(en)\n        clean_sa = clean_line(sa)\n        \n        # Only add if we have valid text in both languages\n        if len(clean_en) > 0 and len(clean_sa) > 0:\n            data.append({\"src\": clean_sa, \"tgt\": clean_en})\n\n    # Convert to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Convert to Hugging Face Dataset\n    full_dataset = Dataset.from_pandas(df)\n    \n    # Split into Train (90%) and Validation (10%)\n    split = full_dataset.train_test_split(test_size=0.1, seed=42)\n    dataset = DatasetDict({\n        'train': split['train'],\n        'validation': split['test']\n    })\n\n    print(f\"\\n‚úì Saamayik Dataset Ready!\")\n    print(f\"Total pairs: {len(dataset['train'])}\")\n    print(f\"Sample Input: {dataset['train'][0]['src']}\")\n    print(f\"Sample Target: {dataset['train'][0]['tgt']}\")\n\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:22:38.721921Z","iopub.execute_input":"2025-12-15T21:22:38.722586Z","iopub.status.idle":"2025-12-15T21:22:38.796017Z","shell.execute_reply.started":"2025-12-15T21:22:38.722562Z","shell.execute_reply":"2025-12-15T21:22:38.795227Z"}},"outputs":[{"name":"stdout","text":"Processing Saamayik dataset...\n\n‚úì Saamayik Dataset Ready!\nTotal pairs: 2173\nSample Input: (‡§¨‡§ø‡§®‡•ç‡§¶‡•Å 1) ‡§≠‡§µ‡§®‡•ç‡§§: ‡§è‡§§‡§Ç ‡§™‡§Ç‡§ï‡•ç‡§§‡•á: ‡§Ö‡§®‡•ç‡§§‡§ø‡§Æ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§Ç ‡§™‡§∞‡•ç‡§Ø‡§®‡•ç‡§§‡§Ç ‡§Ü‡§µ‡§∞‡•ç‡§§‡§Ø‡•á‡§§ ‡•§\nSample Target: (point 1), so you are ready to repeat to the end of the row\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"# Cell 4: Load Model\nmodel_checkpoint = \"google/mt5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\nmodel.to(device)\n\nprint(f\"‚úì Model {model_checkpoint} loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:23:21.687814Z","iopub.execute_input":"2025-12-15T21:23:21.688417Z","iopub.status.idle":"2025-12-15T21:23:26.916180Z","shell.execute_reply.started":"2025-12-15T21:23:21.688385Z","shell.execute_reply":"2025-12-15T21:23:26.915495Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úì Model google/mt5-small loaded.\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"# Cell 5: Preprocessing with Padding Fix\nmax_length = 64\n\ndef preprocess_function(examples):\n    inputs = examples[\"src\"]\n    targets = examples[\"tgt\"]\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        inputs, \n        max_length=max_length, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    # Tokenize targets\n    labels = tokenizer(\n        targets, \n        max_length=max_length, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    # CRITICAL: Replace padding token (0) with -100 to ignore loss on padding\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n        for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply to dataset\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\nprint(\"‚úì Data tokenized and padding masked.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:23:26.917509Z","iopub.execute_input":"2025-12-15T21:23:26.917795Z","iopub.status.idle":"2025-12-15T21:23:27.980837Z","shell.execute_reply.started":"2025-12-15T21:23:26.917768Z","shell.execute_reply":"2025-12-15T21:23:27.980231Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2173 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24970d6ca52472fb8ae115e9b8a3478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/242 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c7e01a1b747457aa7c83af89f48fb7f"}},"metadata":{}},{"name":"stdout","text":"‚úì Data tokenized and padding masked.\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"# Cell 13: Compute Metrics (Safe Fix)\nimport numpy as np\n\n# Load the evaluation metric (BLEU)\nmetric = evaluate.load(\"sacrebleu\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    # 1. Handle tuple output (if any)\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    # 2. THE FIX: Convert -100 to 0 ONLY for decoding\n    #    This allows the tokenizer to read the text without crashing.\n    #    It does NOT affect the model's actual training gradients.\n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n    \n    # 3. Decode predictions\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # 4. Clean up labels (Same fix: -100 -> 0 for readability)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # 5. Clean up text for BLEU\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    # 6. Compute\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    \n    return {\"bleu\": result[\"score\"]}\n\nprint(\"‚úì Metrics function updated. Safe to continue training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:23:32.263326Z","iopub.execute_input":"2025-12-15T21:23:32.263863Z","iopub.status.idle":"2025-12-15T21:23:32.760656Z","shell.execute_reply.started":"2025-12-15T21:23:32.263837Z","shell.execute_reply":"2025-12-15T21:23:32.760013Z"}},"outputs":[{"name":"stdout","text":"‚úì Metrics function updated. Safe to continue training.\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"# Cell 7: Optimized Hyperparameters (Balanced for ~2400 Sentences)\nfrom transformers import Seq2SeqTrainingArguments\nimport transformers\nimport torch\n\n# Clean up memory first\ntorch.cuda.empty_cache()\n\n# 1. Define base arguments\nargs_dict = {\n    \"output_dir\": \"sanskrit-en-model-v2\",\n    \n    # LEARNING RATE: 5e-4 is the \"Sweet Spot\" for fine-tuning.\n    # It learns quickly but doesn't destroy pre-trained knowledge.\n    \"learning_rate\": 5e-4,              \n    \n    # BATCH SIZE: Increased to 8 for speed (Kaggle P100/T4 GPUs can handle this).\n    \"per_device_train_batch_size\": 8,   \n    \"per_device_eval_batch_size\": 8,\n    \n    \"weight_decay\": 0.01,\n    \"save_total_limit\": 2,              # Keep only the last 2 checkpoints\n    \n    # EPOCHS: 20 is ideal for this dataset size.\n    # Math: 2400 lines / 8 batch = 300 steps per epoch. \n    # 20 epochs = 6000 total steps. Perfect for convergence.\n    \"num_train_epochs\": 20,            \n    \n    \"predict_with_generate\": True,\n    \n    # FP16: ENABLED. This speeds up training significantly on Kaggle.\n    \"fp16\": True,                      \n    \n    \"logging_steps\": 50,                # Log less frequently to keep output clean\n    \"report_to\": \"none\",\n    \"save_strategy\": \"epoch\"            # Save a checkpoint at the end of every epoch\n}\n\n# 2. Automatically select the correct strategy parameter (Version check)\nif transformers.__version__ >= \"4.41.0\":\n    args_dict[\"eval_strategy\"] = \"epoch\"\nelse:\n    args_dict[\"evaluation_strategy\"] = \"epoch\"\n\n# 3. Initialize arguments\nargs = Seq2SeqTrainingArguments(**args_dict)\n\nfrom transformers import DataCollatorForSeq2Seq\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\nprint(\"‚úì Optimized Hyperparameters Loaded (20 Epochs, FP16 Enabled).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:30:41.169644Z","iopub.execute_input":"2025-12-15T21:30:41.169904Z","iopub.status.idle":"2025-12-15T21:30:41.433075Z","shell.execute_reply.started":"2025-12-15T21:30:41.169886Z","shell.execute_reply":"2025-12-15T21:30:41.432347Z"}},"outputs":[{"name":"stdout","text":"‚úì Optimized Hyperparameters Loaded (20 Epochs, FP16 Enabled).\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"# Cell: Verify Alignment\nprint(\"Checking data alignment at different points...\")\n\n# Check index 0 (Start)\nprint(f\"--- Index 0 ---\")\nprint(f\"San: {dataset['train'][0]['src']}\")\nprint(f\"Eng: {dataset['train'][0]['tgt']}\")\n\n# Check index 100 (Early)\nprint(f\"\\n--- Index 100 ---\")\nprint(f\"San: {dataset['train'][145]['src']}\")\nprint(f\"Eng: {dataset['train'][145]['tgt']}\")\n\n# Check index 1000 (Middle)\nprint(f\"\\n--- Index 1000 ---\")\nprint(f\"San: {dataset['train'][1000]['src']}\")\nprint(f\"Eng: {dataset['train'][1000]['tgt']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:30:44.555522Z","iopub.execute_input":"2025-12-15T21:30:44.555809Z","iopub.status.idle":"2025-12-15T21:30:44.562467Z","shell.execute_reply.started":"2025-12-15T21:30:44.555787Z","shell.execute_reply":"2025-12-15T21:30:44.561733Z"}},"outputs":[{"name":"stdout","text":"Checking data alignment at different points...\n--- Index 0 ---\nSan: (‡§¨‡§ø‡§®‡•ç‡§¶‡•Å 1) ‡§≠‡§µ‡§®‡•ç‡§§: ‡§è‡§§‡§Ç ‡§™‡§Ç‡§ï‡•ç‡§§‡•á: ‡§Ö‡§®‡•ç‡§§‡§ø‡§Æ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§Ç ‡§™‡§∞‡•ç‡§Ø‡§®‡•ç‡§§‡§Ç ‡§Ü‡§µ‡§∞‡•ç‡§§‡§Ø‡•á‡§§ ‡•§\nEng: (point 1), so you are ready to repeat to the end of the row\n\n--- Index 100 ---\nSan: ‡§Ø‡§§‡•ã ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§Ø‡§æ ‡§Ø‡•á ‡§Æ‡§π‡§æ‡§Ø‡§æ‡§ú‡§ï‡§æ ‡§®‡§ø‡§∞‡•Ç‡§™‡•ç‡§Ø‡§®‡•ç‡§§‡•á ‡§§‡•á ‡§¶‡•å‡§∞‡•ç‡§¨‡•ç‡§¨‡§≤‡•ç‡§Ø‡§Ø‡•Å‡§ï‡•ç‡§§‡§æ ‡§Æ‡§æ‡§®‡§µ‡§æ‡§É ‡§ï‡§ø‡§®‡•ç‡§§‡•Å ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§§‡§É ‡§™‡§∞‡§Ç ‡§∂‡§™‡§•‡§Ø‡•Å‡§ï‡•ç‡§§‡•á‡§® ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•á‡§® ‡§Ø‡•ã ‡§Æ‡§π‡§æ‡§Ø‡§æ‡§ú‡§ï‡•ã ‡§®‡§ø‡§∞‡•Ç‡§™‡§ø‡§§‡§É ‡§∏‡•ã ‡§Ω‡§®‡§®‡•ç‡§§‡§ï‡§æ‡§≤‡§æ‡§∞‡•ç‡§•‡§Ç ‡§∏‡§ø‡§¶‡•ç‡§ß‡§É ‡§™‡•Å‡§§‡•ç‡§∞ ‡§è‡§µ‡•§\nEng: For the law maketh men high priests which have infirmity; but the word of the oath, which was since the law, maketh the Son, who is consecrated for evermore.\n\n--- Index 1000 ---\nSan: ‡§™‡§∂‡•ç‡§ö‡§æ‡§§‡•ç  Finish  ‡§á‡§§‡•ç‡§Ø‡§§‡•ç‡§∞ ‡§ï‡•ç‡§≤‡§ø‡§ï‡•ç ‡§ï‡§∞‡•ã‡§§‡•Å ‡•§\nEng: Then click on Finish.\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"# Cell 8: Start Training\ntrainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\nprint(\"üöÄ Starting training...\")\ntrainer.train()\nprint(\"‚úì Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:30:54.134236Z","iopub.execute_input":"2025-12-15T21:30:54.134936Z","iopub.status.idle":"2025-12-15T22:01:46.748744Z","shell.execute_reply.started":"2025-12-15T21:30:54.134911Z","shell.execute_reply":"2025-12-15T22:01:46.747689Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/662804619.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2720' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2720/2720 30:51, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.995300</td>\n      <td>3.636428</td>\n      <td>3.960434</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.569100</td>\n      <td>3.469731</td>\n      <td>4.930482</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.240500</td>\n      <td>3.388294</td>\n      <td>5.145590</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.100300</td>\n      <td>3.336191</td>\n      <td>5.845201</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.813200</td>\n      <td>3.303050</td>\n      <td>6.618728</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.703300</td>\n      <td>3.313157</td>\n      <td>6.302028</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.514500</td>\n      <td>3.282174</td>\n      <td>7.374906</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.367000</td>\n      <td>3.316272</td>\n      <td>7.352389</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.259500</td>\n      <td>3.327850</td>\n      <td>7.800122</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.080900</td>\n      <td>3.333607</td>\n      <td>8.037683</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.012500</td>\n      <td>3.351918</td>\n      <td>8.014275</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.923200</td>\n      <td>3.342384</td>\n      <td>8.725565</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.876800</td>\n      <td>3.399497</td>\n      <td>7.894362</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.796600</td>\n      <td>3.443575</td>\n      <td>8.993514</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.726400</td>\n      <td>3.490010</td>\n      <td>8.226500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.687600</td>\n      <td>3.471685</td>\n      <td>8.383922</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.574100</td>\n      <td>3.493363</td>\n      <td>8.521951</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.533000</td>\n      <td>3.523443</td>\n      <td>8.794049</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.548300</td>\n      <td>3.527826</td>\n      <td>8.069685</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.524000</td>\n      <td>3.534201</td>\n      <td>8.606569</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úì Training complete.\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"# Cell 9: Save Model\nmodel.save_pretrained(\"./final_sanskrit_model\")\ntokenizer.save_pretrained(\"./final_sanskrit_model\")\nprint(\"‚úì Model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T22:02:15.160155Z","iopub.execute_input":"2025-12-15T22:02:15.160467Z","iopub.status.idle":"2025-12-15T22:02:18.341483Z","shell.execute_reply.started":"2025-12-15T22:02:15.160444Z","shell.execute_reply":"2025-12-15T22:02:18.340496Z"}},"outputs":[{"name":"stdout","text":"‚úì Model saved.\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# Cell 10: Test the Translator\ndef translate(text):\n    # Tokenize\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    \n    # Generate\n    outputs = model.generate(\n        inputs[\"input_ids\"], \n        max_length=64, \n        num_beams=4, \n        early_stopping=True,\n        no_repeat_ngram_size=2\n    )\n    \n    # Decode\n    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return translation\n\n# Test sentences\ntests = [\n    \"‡§Ö‡§§‡§É ‡§µ‡§Ø‡§Æ‡§ø‡§¶‡§Ç ‡§ï‡§®‡•ç‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ü‡§∞‡•ç ‡§Ö‡§∏‡•ç‡§Ø ‡§™‡•ç‡§∞‡§•‡§Æ‡§Ç ‡§™‡§ô‡•ç‡§ï‡•ç‡§§‡§ø‡§Ç ‡§ï‡•Å‡§∞‡•ç‡§Æ‡§É ‡•§\",   # Rama goes to the forest\n    \"‡§§‡•ç‡§µ‡§Ç ‡§ö‡§ø‡§§‡•ç‡§∞‡§Æ‡•ç ‡§Ö‡§™‡§∂‡•ç‡§Ø‡§É \", # I go to school\n    \"‡§§‡•á ‡§µ‡•Ä‡§∞‡§æ‡§É ‡•§\"    # Dharma protects those who protect it\n]\n\nprint(\"-\" * 30)\nfor t in tests:\n    print(f\"Sanskrit: {t}\")\n    print(f\"English:  {translate(t)}\")\n    print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T05:08:34.347392Z","iopub.execute_input":"2025-12-16T05:08:34.347586Z","iopub.status.idle":"2025-12-16T05:08:34.416277Z","shell.execute_reply.started":"2025-12-16T05:08:34.347561Z","shell.execute_reply":"2025-12-16T05:08:34.415280Z"}},"outputs":[{"name":"stdout","text":"------------------------------\nSanskrit: ‡§Ö‡§§‡§É ‡§µ‡§Ø‡§Æ‡§ø‡§¶‡§Ç ‡§ï‡§®‡•ç‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ü‡§∞‡•ç ‡§Ö‡§∏‡•ç‡§Ø ‡§™‡•ç‡§∞‡§•‡§Æ‡§Ç ‡§™‡§ô‡•ç‡§ï‡•ç‡§§‡§ø‡§Ç ‡§ï‡•Å‡§∞‡•ç‡§Æ‡§É ‡•§\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2886002845.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sanskrit: {t}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"English:  {translate(t)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2886002845.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Cell 11: Save Model for Download\nimport shutil\nimport os\nfrom IPython.display import FileLink\n\n# 1. Define where to save\nsave_path = \"sanskrit_translator_final\"\n\nprint(f\"Saving model to {save_path}...\")\n\n# 2. Save Model and Tokenizer\n#    (We save both because the model needs the exact same tokenizer to work)\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\nprint(\"‚úì Model and Tokenizer saved.\")\n\n# 3. Zip the folder\n#    (Kaggle makes it easier to download one zip file than 5-6 separate files)\nzip_filename = \"sanskrit_model.zip\"\nshutil.make_archive(\"sanskrit_model\", 'zip', save_path)\n\nprint(f\"‚úì Zipped into {zip_filename}\")\n\n# 4. Create a Download Link\nprint(\"\\n‚¨áÔ∏è Click the link below to download your model:\")\ndisplay(FileLink(zip_filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T22:08:56.401786Z","iopub.execute_input":"2025-12-15T22:08:56.402102Z","iopub.status.idle":"2025-12-15T22:10:11.838605Z","shell.execute_reply.started":"2025-12-15T22:08:56.402080Z","shell.execute_reply":"2025-12-15T22:10:11.837932Z"}},"outputs":[{"name":"stdout","text":"Saving model to sanskrit_translator_final...\n‚úì Model and Tokenizer saved.\n‚úì Zipped into sanskrit_model.zip\n\n‚¨áÔ∏è Click the link below to download your model:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/sanskrit_model.zip","text/html":"<a href='sanskrit_model.zip' target='_blank'>sanskrit_model.zip</a><br>"},"metadata":{}}],"execution_count":112},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\n\ndef clean_line(line):\n    # Remove the tags and leading/trailing whitespace\n    cleaned = re.sub(r'\\', '', line).strip()\n    return cleaned\n\ndef prepare_saamayik(en_path, sa_path, output_csv=\"saamayik_cleaned.csv\"):\n    # Read files\n    with open(en_path, \"r\", encoding=\"utf-8\") as f_en, \\\n         open(sa_path, \"r\", encoding=\"utf-8\") as f_sa:\n        \n        en_lines = f_en.readlines()\n        sa_lines = f_sa.readlines()\n\n    # Ensure alignment (rudimentary check)\n    if len(en_lines) != len(sa_lines):\n        print(f\"Warning: Line counts differ! En: {len(en_lines)}, Sa: {len(sa_lines)}\")\n        # You might need more advanced alignment if lines are missing, \n        # but Saamayik is usually 1:1.\n\n    data = []\n    \n    # Process lines\n    for en, sa in zip(en_lines, sa_lines):\n        clean_en = clean_line(en)\n        clean_sa = clean_line(sa)\n        \n        # Only add if both have content\n        if clean_en and clean_sa:\n            data.append({\"English\": clean_en, \"Sanskrit\": clean_sa})\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n    \n    # Save to CSV\n    df.to_csv(output_csv, index=False)\n    print(f\"Successfully saved {len(df)} pairs to {output_csv}\")\n    return df\n\n# Usage (assuming your files are named dev.en and dev.sa)\n# df = prepare_saamayik('dev.en', 'dev.sa')\n# print(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T21:13:54.580812Z","iopub.execute_input":"2025-12-15T21:13:54.581564Z","iopub.status.idle":"2025-12-15T21:13:54.588094Z","shell.execute_reply.started":"2025-12-15T21:13:54.581535Z","shell.execute_reply":"2025-12-15T21:13:54.587205Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_47/1700770016.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    cleaned = re.sub(r'\\', '', line).strip()\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 6)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 6) (1700770016.py, line 6)","output_type":"error"}],"execution_count":86},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}